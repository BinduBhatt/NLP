{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b83c8c55",
   "metadata": {},
   "source": [
    "## Working Instructions\n",
    "#### Please change the paths as required to locate the files on respective systems. Thereafter, please run the complete file as a script. Run all cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bd3538a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required files\n",
    "import pandas as pd     # to read the input excel sheet\n",
    "from bs4 import BeautifulSoup    # for scraping data from the website and navigating their structure\n",
    "import requests  # to fetch the html content.\n",
    "import os       # importing Operating System Interface i.e. os library\n",
    "import string    # to remove punctuations\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize    # for tokenization\n",
    "import re        # for pattern search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8a1819c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the excel sheet with URLs\n",
    "df=pd.read_excel(r'C:\\Users\\Admin\\Desktop\\Test Assignment\\20211030 Test Assignment\\Input.xlsx')\n",
    "name=df.URL_ID  # defining name to the url_id column\n",
    "URL=df['URL'].str.strip() # striping the trailing space in the URL column as it is causing issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8ae601ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error message: https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/\n",
      "'NoneType' object has no attribute 'text'\n",
      "Error message: https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/\n",
      "'NoneType' object has no attribute 'text'\n"
     ]
    }
   ],
   "source": [
    "# Creating  a for loop here which can navigate through all the URLs in the sheet to extract the data.\n",
    "for i, j in zip(URL,name):    # Zipping the columns \n",
    "    try:\n",
    "        soup=BeautifulSoup(requests.get(i).text, 'lxml')    # requesting the html content from site and parsing it via 'lxml'\n",
    "        Title=soup.find('title').text                       # extracting the title\n",
    "        article=soup.find(attrs={'class': 'td-post-content'}).text  # extracting the article content\n",
    "        result=Title+article                                        # concatinating title and article in one variable\n",
    "        para=result.replace('\\n', '').replace('\\xa0','')            # replacing the non required expressions from the article\n",
    "    \n",
    "        with open('%s.txt'%j,'w', encoding='utf-8') as file:        # opening a text file to write the content extracted in para\n",
    "            file.write(para)\n",
    "            file.close                                              # closing the file.\n",
    "    except Exception as e:                                          # tracking the error, if any occurs during the execution\n",
    "        print('Error message:', i)\n",
    "        print(e)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577861d9",
   "metadata": {},
   "source": [
    "Output Result:\n",
    "\n",
    "\n",
    "Row 37 - blackassign0036 - 'Ooops... Error 404 \\' 'Sorry, but the page you are looking for doesn't exist.'\n",
    "https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/\n",
    "\n",
    "Row 50 - blackassign0049 - 'Ooops... Error 404 \\' 'Sorry, but the page you are looking for doesn't exist.'\n",
    "https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39df5efe",
   "metadata": {},
   "source": [
    "## Stop Words File "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "65477d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing and formatting the stopwords in one variable from the notepad\n",
    "import os   # importing Operating System Interface i.e. os library\n",
    "\n",
    "# Defining a function to collect stop words from different files in the file path by navigating through all the text file\n",
    "def stop_file(file_path1):            \n",
    "    file_names=os.listdir(file_path1)     # getting the list of files in the variable\n",
    "    compiled_lines=[]                    # adding all the final words in this variable\n",
    "    \n",
    "    # looping for all the files\n",
    "    for file_name in file_names:\n",
    "        file_path_i=os.path.join(file_path1, file_name)  # full path of the individual files\n",
    "        \n",
    "        # Opening the file to read\n",
    "        with open(file_path_i,'r',encoding='latin-1') as file:\n",
    "            lines=file.readlines()     # reading the content of the file \n",
    "            lines=[line.strip() for line in lines]  # strip newline characters from each line\n",
    "            compiled_lines.extend(lines)      #adding the line to compiled lines\n",
    "    return compiled_lines\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f9ad33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the above defined function by providing the path of the directory where files are saved\n",
    "compiled_lines=stop_file(r'C:\\Users\\Admin\\Desktop\\Test Assignment\\20211030 Test Assignment\\StopWords')\n",
    "\n",
    "# replacing some hyperlinks in the data\n",
    "compiled_lines[compiled_lines.index('JAMES  | http://www.census.gov/genealogy/names/dist.male.first')]='James'\n",
    "compiled_lines[compiled_lines.index('LI  |  http://en.wikipedia.org/wiki/List_of_most_common_surnames#China')] = 'LI'\n",
    "\n",
    "#replacing some characters from the compiled file and setting all the words in lower case with the help of list comprehension\n",
    "modified_lines=[i.replace('|',',').replace('(', '').replace(')', '').replace('.', '') for i in compiled_lines]\n",
    "stopwords_modified=[i.lower() for i in modified_lines]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df02061d",
   "metadata": {},
   "source": [
    "## Data Cleaning & Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b44af79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining function to clean the data, remove stop_words, tokenizing\n",
    "def data_clean(file_path):\n",
    "    file_names= os.listdir(file_path)           # getting the list of files in the variable\n",
    "    cleaned_files=[]                             # initializing the variable to store namees of all the clean files\n",
    "    final_words=[]                              # initializing the variable to add the final words\n",
    "\n",
    "    \n",
    "    for file_name in file_names:               # iterating through the file names directory\n",
    "        file_name_individual=os.path.join(file_path, file_name)      # full path of the individual file\n",
    "        cleanfile_name=os.path.splitext(file_name)[0]+'_clean.txt'   # setting the names of the clean files by using split extension\n",
    "        \n",
    "        if file_name.endswith('.txt'):                 # making sure that it only picks text files\n",
    "            with open(file_name_individual,'r',encoding='utf-8') as text:            # opening individual files to read for text preprocessing\n",
    "                file=text.read().lower()                                             \n",
    "                tokenized_sent=(sent_tokenize(file))            # tokenizing the sentences\n",
    "                num_sent=len(tokenized_sent)                    # counting the number of sentences\n",
    "               \n",
    "                \n",
    "                clean_data=file.translate(str.maketrans('', '', string.punctuation)) \n",
    "                tokenized_words=(word_tokenize(clean_data))\n",
    "                final_words=[word for word in tokenized_words if word not in stopwords_modified]\n",
    "                count_of_words=len(final_words)                      # counting the number of words\n",
    "                \n",
    "            cleanfile_path=os.path.join(output_path, cleanfile_name)      # full path of the individual clean files\n",
    "            with open(cleanfile_path, 'w', encoding='utf-8') as output_file:  # opening individual files to write\n",
    "                for item in final_words:\n",
    "                    output_file.write(f'\"{item}\",' )  # writing all the words in clean file\n",
    "            \n",
    "            cleaned_files.append((cleanfile_name, num_sent, count_of_words))    # appending the names of the clean file\n",
    "                \n",
    "    return cleaned_files    # output will be the list of cleaned files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b812f2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r'C:\\Users\\Admin\\BlackcOffer- Test Assignment'    # file path for text files for text preprocessing\n",
    "output_path = r'C:\\Users\\Admin\\BlackcOffer- Test Assignment\\Clean'   # path for cleaned_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ec0c2ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('blackassign0001_clean.txt', 57, 489), ('blackassign0002_clean.txt', 64, 708), ('blackassign0003_clean.txt', 46, 613), ('blackassign0004_clean.txt', 33, 592), ('blackassign0005_clean.txt', 26, 343), ('blackassign0006_clean.txt', 59, 1026), ('blackassign0007_clean.txt', 51, 652), ('blackassign0008_clean.txt', 36, 475), ('blackassign0009_clean.txt', 39, 581), ('blackassign0010_clean.txt', 108, 1639), ('blackassign0011_clean.txt', 44, 779), ('blackassign0012_clean.txt', 56, 824), ('blackassign0013_clean.txt', 19, 290), ('blackassign0014_clean.txt', 62, 521), ('blackassign0015_clean.txt', 44, 640), ('blackassign0016_clean.txt', 44, 640), ('blackassign0017_clean.txt', 43, 577), ('blackassign0018_clean.txt', 33, 562), ('blackassign0019_clean.txt', 73, 828), ('blackassign0020_clean.txt', 22, 227), ('blackassign0021_clean.txt', 41, 520), ('blackassign0022_clean.txt', 23, 176), ('blackassign0023_clean.txt', 54, 585), ('blackassign0024_clean.txt', 14, 277), ('blackassign0025_clean.txt', 18, 457), ('blackassign0026_clean.txt', 36, 468), ('blackassign0027_clean.txt', 42, 558), ('blackassign0028_clean.txt', 30, 560), ('blackassign0029_clean.txt', 50, 963), ('blackassign0030_clean.txt', 66, 583), ('blackassign0031_clean.txt', 66, 829), ('blackassign0032_clean.txt', 79, 645), ('blackassign0033_clean.txt', 55, 805), ('blackassign0034_clean.txt', 45, 576), ('blackassign0035_clean.txt', 41, 335), ('blackassign0037_clean.txt', 29, 334), ('blackassign0038_clean.txt', 68, 943), ('blackassign0039_clean.txt', 88, 1004), ('blackassign0040_clean.txt', 40, 580), ('blackassign0041_clean.txt', 61, 608), ('blackassign0042_clean.txt', 34, 670), ('blackassign0043_clean.txt', 50, 739), ('blackassign0044_clean.txt', 17, 253), ('blackassign0045_clean.txt', 8, 718), ('blackassign0046_clean.txt', 32, 441), ('blackassign0047_clean.txt', 19, 355), ('blackassign0048_clean.txt', 2, 146), ('blackassign0050_clean.txt', 42, 660), ('blackassign0051_clean.txt', 24, 319), ('blackassign0052_clean.txt', 87, 785), ('blackassign0053_clean.txt', 27, 320), ('blackassign0054_clean.txt', 13, 195), ('blackassign0055_clean.txt', 26, 547), ('blackassign0056_clean.txt', 12, 313), ('blackassign0057_clean.txt', 23, 152), ('blackassign0058_clean.txt', 8, 86), ('blackassign0059_clean.txt', 55, 350), ('blackassign0060_clean.txt', 6, 79), ('blackassign0061_clean.txt', 36, 601), ('blackassign0062_clean.txt', 20, 241), ('blackassign0063_clean.txt', 40, 454), ('blackassign0064_clean.txt', 50, 819), ('blackassign0065_clean.txt', 42, 719), ('blackassign0066_clean.txt', 70, 639), ('blackassign0067_clean.txt', 54, 319), ('blackassign0068_clean.txt', 62, 516), ('blackassign0069_clean.txt', 14, 177), ('blackassign0070_clean.txt', 41, 741), ('blackassign0071_clean.txt', 58, 406), ('blackassign0072_clean.txt', 60, 488), ('blackassign0073_clean.txt', 31, 575), ('blackassign0074_clean.txt', 43, 682), ('blackassign0075_clean.txt', 61, 433), ('blackassign0076_clean.txt', 37, 648), ('blackassign0077_clean.txt', 21, 359), ('blackassign0078_clean.txt', 28, 583), ('blackassign0079_clean.txt', 46, 774), ('blackassign0080_clean.txt', 159, 1632), ('blackassign0081_clean.txt', 61, 772), ('blackassign0082_clean.txt', 47, 798), ('blackassign0083_clean.txt', 4, 71), ('blackassign0084_clean.txt', 36, 449), ('blackassign0085_clean.txt', 105, 683), ('blackassign0086_clean.txt', 66, 809), ('blackassign0087_clean.txt', 35, 482), ('blackassign0088_clean.txt', 36, 887), ('blackassign0089_clean.txt', 33, 447), ('blackassign0090_clean.txt', 65, 511), ('blackassign0091_clean.txt', 29, 463), ('blackassign0092_clean.txt', 27, 656), ('blackassign0093_clean.txt', 5, 78), ('blackassign0094_clean.txt', 46, 525), ('blackassign0095_clean.txt', 18, 324), ('blackassign0096_clean.txt', 44, 549), ('blackassign0097_clean.txt', 27, 415), ('blackassign0098_clean.txt', 9, 215), ('blackassign0099_clean.txt', 22, 272), ('blackassign0100_clean.txt', 18, 526)]\n"
     ]
    }
   ],
   "source": [
    "# applying the function for data clean\n",
    "cleaned_files= data_clean(file_path)   \n",
    "print(cleaned_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cabb35",
   "metadata": {},
   "source": [
    "# Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "37248f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_analysis(location,path, path1):\n",
    "    final_files=os.listdir(location)     # directory for files for text analysis\n",
    "    result=[]     # creating variable to store the results\n",
    "    \n",
    "    \n",
    "    with open(path,'r', encoding='latin-1') as positive_file,open(path1, 'r', encoding='latin-1') as negative_file:\n",
    "        positive_words=set(positive_file.read().split())    # splitting the words \n",
    "        negative_words=set(negative_file.read().split())\n",
    "       \n",
    "        \n",
    "        for final_file in final_files:\n",
    "            final_file_i=os.path.join(location,final_file)    # full path for individual files\n",
    "            \n",
    "               \n",
    "             \n",
    "            with open(final_file_i,'r', encoding='latin-1') as p:     # opening and reading the individual files\n",
    "                content = p.read()\n",
    "                words = content.replace('\"', '').split(',')      # as the file contain quotes and commas, replacing quotes and splitting by ','\n",
    "                positive_score=0                                 # initializing scores\n",
    "                negative_score=0\n",
    "                for word in words:\n",
    "                    if word in positive_words:                  # applying conditions\n",
    "                        positive_score +=1\n",
    "                            \n",
    "                    elif word in negative_words:\n",
    "                        negative_score +=1\n",
    "                                              \n",
    "                          \n",
    "                result.append((positive_score, negative_score, final_file))     # appending results to final lists\n",
    "            \n",
    "    return result      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d19160a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# providing paths for files\n",
    "path= r'C:\\Users\\Admin\\Desktop\\Test Assignment\\20211030 Test Assignment\\MasterDictionary\\positive-words.txt'\n",
    "path1 = r'C:\\Users\\Admin\\Desktop\\Test Assignment\\20211030 Test Assignment\\MasterDictionary\\negative-words.txt'\n",
    "location=r'C:\\Users\\Admin\\BlackcOffer- Test Assignment\\Clean'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "849d2d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_neg_score=text_analysis(location,path, path1)   # retrieving the output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e69ca4",
   "metadata": {},
   "source": [
    "#### Polarity, Subjectivity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2cb4543e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the function for polarity score\n",
    "def polarity_score(positive_score, negative_score, file_name):\n",
    "    polarity_score=[]                             #initializing the list to collect all scores\n",
    "    for positive_score, negative_score, file_name in zip(pos_score, neg_score, file_names):    # looping through all the scores and files\n",
    "        pscore=round((positive_score-negative_score)/((positive_score+negative_score)+ 0.000001),3)\n",
    "        polarity_score.append((pscore,file_name))           # appending to the list\n",
    "    return polarity_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1b3b2dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_score=[positive_score for positive_score,_,_ in pos_neg_score]   # defining positive and negative score list\n",
    "neg_score=[negative_score for _,negative_score,_ in pos_neg_score]\n",
    "file_names=[file_name for _,_, file_name in pos_neg_score]                # file names list\n",
    "\n",
    "polarity_score=polarity_score(pos_score, neg_score, file_names)     # calling the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "127b48fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function for counting the number of characters in a file.\n",
    "def count_characters(location):\n",
    "        output=[]\n",
    "        clean_file_names=os.listdir(location)\n",
    "        for clean_file in clean_file_names:\n",
    "            clean_file_i=os.path.join(location,clean_file)\n",
    "            \n",
    "            with open(clean_file_i,'r', encoding='latin-1') as p:\n",
    "                words=[word.strip() for word in p.read()]\n",
    "                total_characters=len(words)                             # counting the characters\n",
    "                output.append((total_characters, clean_file))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "44247848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the function by giving location as argument which is already defined earlier\n",
    "output=count_characters(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b0c64909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function for subjectivity score\n",
    "def sub_score(pos_score, neg_score, file_names, num_of_words):\n",
    "    s_scores = []\n",
    "    \n",
    "    # zipping the lists together for iterations\n",
    "    for pos, neg, file_name, num_words in zip(pos_score, neg_score, file_names, num_of_words):  \n",
    "        sub_score = round((pos + neg) / (num_words + 0.000001), 3)\n",
    "        s_scores.append((sub_score, file_name))\n",
    "    return s_scores\n",
    "\n",
    "#unpacking each tuple to get the third value and creating a new list with extracted values\n",
    "num_of_words = [count_of_words for _, _, count_of_words in cleaned_files]\n",
    "\n",
    "# calling the function\n",
    "subjectivity_score = sub_score(pos_score, neg_score, file_names, num_of_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7198f5",
   "metadata": {},
   "source": [
    "### Analysis of Readability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef691fb7",
   "metadata": {},
   "source": [
    "#### Average Sentence Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ee2fa525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function for average sent length which is same as avg words per sentence.\n",
    "def average_sent_len(words,count_sent, file):\n",
    "    average_length=round(words/count_sent,3)\n",
    "    return average_length, file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ae26c923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8.579, 'blackassign0001_clean.txt'), (11.062, 'blackassign0002_clean.txt'), (13.326, 'blackassign0003_clean.txt'), (17.939, 'blackassign0004_clean.txt'), (13.192, 'blackassign0005_clean.txt'), (17.39, 'blackassign0006_clean.txt'), (12.784, 'blackassign0007_clean.txt'), (13.194, 'blackassign0008_clean.txt'), (14.897, 'blackassign0009_clean.txt'), (15.176, 'blackassign0010_clean.txt'), (17.705, 'blackassign0011_clean.txt'), (14.714, 'blackassign0012_clean.txt'), (15.263, 'blackassign0013_clean.txt'), (8.403, 'blackassign0014_clean.txt'), (14.545, 'blackassign0015_clean.txt'), (14.545, 'blackassign0016_clean.txt'), (13.419, 'blackassign0017_clean.txt'), (17.03, 'blackassign0018_clean.txt'), (11.342, 'blackassign0019_clean.txt'), (10.318, 'blackassign0020_clean.txt'), (12.683, 'blackassign0021_clean.txt'), (7.652, 'blackassign0022_clean.txt'), (10.833, 'blackassign0023_clean.txt'), (19.786, 'blackassign0024_clean.txt'), (25.389, 'blackassign0025_clean.txt'), (13.0, 'blackassign0026_clean.txt'), (13.286, 'blackassign0027_clean.txt'), (18.667, 'blackassign0028_clean.txt'), (19.26, 'blackassign0029_clean.txt'), (8.833, 'blackassign0030_clean.txt'), (12.561, 'blackassign0031_clean.txt'), (8.165, 'blackassign0032_clean.txt'), (14.636, 'blackassign0033_clean.txt'), (12.8, 'blackassign0034_clean.txt'), (8.171, 'blackassign0035_clean.txt'), (11.517, 'blackassign0037_clean.txt'), (13.868, 'blackassign0038_clean.txt'), (11.409, 'blackassign0039_clean.txt'), (14.5, 'blackassign0040_clean.txt'), (9.967, 'blackassign0041_clean.txt'), (19.706, 'blackassign0042_clean.txt'), (14.78, 'blackassign0043_clean.txt'), (14.882, 'blackassign0044_clean.txt'), (89.75, 'blackassign0045_clean.txt'), (13.781, 'blackassign0046_clean.txt'), (18.684, 'blackassign0047_clean.txt'), (73.0, 'blackassign0048_clean.txt'), (15.714, 'blackassign0050_clean.txt'), (13.292, 'blackassign0051_clean.txt'), (9.023, 'blackassign0052_clean.txt'), (11.852, 'blackassign0053_clean.txt'), (15.0, 'blackassign0054_clean.txt'), (21.038, 'blackassign0055_clean.txt'), (26.083, 'blackassign0056_clean.txt'), (6.609, 'blackassign0057_clean.txt'), (10.75, 'blackassign0058_clean.txt'), (6.364, 'blackassign0059_clean.txt'), (13.167, 'blackassign0060_clean.txt'), (16.694, 'blackassign0061_clean.txt'), (12.05, 'blackassign0062_clean.txt'), (11.35, 'blackassign0063_clean.txt'), (16.38, 'blackassign0064_clean.txt'), (17.119, 'blackassign0065_clean.txt'), (9.129, 'blackassign0066_clean.txt'), (5.907, 'blackassign0067_clean.txt'), (8.323, 'blackassign0068_clean.txt'), (12.643, 'blackassign0069_clean.txt'), (18.073, 'blackassign0070_clean.txt'), (7.0, 'blackassign0071_clean.txt'), (8.133, 'blackassign0072_clean.txt'), (18.548, 'blackassign0073_clean.txt'), (15.86, 'blackassign0074_clean.txt'), (7.098, 'blackassign0075_clean.txt'), (17.514, 'blackassign0076_clean.txt'), (17.095, 'blackassign0077_clean.txt'), (20.821, 'blackassign0078_clean.txt'), (16.826, 'blackassign0079_clean.txt'), (10.264, 'blackassign0080_clean.txt'), (12.656, 'blackassign0081_clean.txt'), (16.979, 'blackassign0082_clean.txt'), (17.75, 'blackassign0083_clean.txt'), (12.472, 'blackassign0084_clean.txt'), (6.505, 'blackassign0085_clean.txt'), (12.258, 'blackassign0086_clean.txt'), (13.771, 'blackassign0087_clean.txt'), (24.639, 'blackassign0088_clean.txt'), (13.545, 'blackassign0089_clean.txt'), (7.862, 'blackassign0090_clean.txt'), (15.966, 'blackassign0091_clean.txt'), (24.296, 'blackassign0092_clean.txt'), (15.6, 'blackassign0093_clean.txt'), (11.413, 'blackassign0094_clean.txt'), (18.0, 'blackassign0095_clean.txt'), (12.477, 'blackassign0096_clean.txt'), (15.37, 'blackassign0097_clean.txt'), (23.889, 'blackassign0098_clean.txt'), (12.364, 'blackassign0099_clean.txt'), (29.222, 'blackassign0100_clean.txt')]\n"
     ]
    }
   ],
   "source": [
    "num_words=[count_of_words for _,_,count_of_words in cleaned_files]\n",
    "sent=[num_sent for _,num_sent,_ in cleaned_files]\n",
    "average_len=[average_sent_len(words,count_sent, file) for words,count_sent, file in zip(num_words, sent, file_names)] \n",
    "print(average_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265c36bf",
   "metadata": {},
   "source": [
    "#### Complex Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bf119873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for calculating number of complex words per file\n",
    "vowels = ['a', 'e', 'i', 'o', 'u']\n",
    "\n",
    "def cal_complex_words(location):\n",
    "    complex_words_count = []\n",
    "   \n",
    "    clean_file_names = os.listdir(location)\n",
    "    for clean_file in clean_file_names:\n",
    "        clean_file_i = os.path.join(location, clean_file)    \n",
    "            \n",
    "        with open(clean_file_i, 'r', encoding='latin-1') as p:\n",
    "            text=p.read()\n",
    "            content=text.replace('\"','').split(',')\n",
    "            words = [word for word in content]\n",
    "            complex_words=0\n",
    "            for word in words:  \n",
    "                count = 0\n",
    "\n",
    "                for letter in word:\n",
    "                    if letter.lower() in vowels:\n",
    "                        count += 1\n",
    "                    if word.endswith(('es', 'ed')):\n",
    "                        count -= 1\n",
    "                    if count>2:\n",
    "                        complex_words+=1\n",
    "                        break\n",
    "                    \n",
    "            complex_words_count.append((complex_words, clean_file))\n",
    "           \n",
    "                        \n",
    "    return complex_words_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a90852ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling function\n",
    "num=cal_complex_words(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "52a08d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpacking to retrieve targeted value for calculation and creating new lists\n",
    "complex_num=[complex_words for complex_words,_ in num ]\n",
    "clean_files=[clean_file for _,clean_file in num]\n",
    "num_words=[count_of_words for _,_,count_of_words in cleaned_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0143dddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the percentage of complex words\n",
    "def Complex_percent(comp_words, words, file):\n",
    "    comp_word_per=round((comp_words/words)*100,3)\n",
    "    return comp_word_per, file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "62322446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(39.059, 'blackassign0001_clean.txt'), (48.023, 'blackassign0002_clean.txt'), (53.834, 'blackassign0003_clean.txt'), (55.743, 'blackassign0004_clean.txt'), (46.064, 'blackassign0005_clean.txt'), (51.462, 'blackassign0006_clean.txt'), (47.393, 'blackassign0007_clean.txt'), (54.526, 'blackassign0008_clean.txt'), (56.282, 'blackassign0009_clean.txt'), (51.19, 'blackassign0010_clean.txt'), (56.098, 'blackassign0011_clean.txt'), (60.194, 'blackassign0012_clean.txt'), (59.655, 'blackassign0013_clean.txt'), (46.257, 'blackassign0014_clean.txt'), (50.313, 'blackassign0015_clean.txt'), (50.313, 'blackassign0016_clean.txt'), (47.834, 'blackassign0017_clean.txt'), (52.669, 'blackassign0018_clean.txt'), (52.053, 'blackassign0019_clean.txt'), (40.529, 'blackassign0020_clean.txt'), (54.038, 'blackassign0021_clean.txt'), (45.455, 'blackassign0022_clean.txt'), (44.615, 'blackassign0023_clean.txt'), (50.542, 'blackassign0024_clean.txt'), (33.917, 'blackassign0025_clean.txt'), (42.094, 'blackassign0026_clean.txt'), (34.767, 'blackassign0027_clean.txt'), (46.429, 'blackassign0028_clean.txt'), (52.025, 'blackassign0029_clean.txt'), (39.28, 'blackassign0030_clean.txt'), (50.663, 'blackassign0031_clean.txt'), (46.977, 'blackassign0032_clean.txt'), (42.981, 'blackassign0033_clean.txt'), (43.924, 'blackassign0034_clean.txt'), (34.925, 'blackassign0035_clean.txt'), (42.814, 'blackassign0037_clean.txt'), (45.069, 'blackassign0038_clean.txt'), (40.936, 'blackassign0039_clean.txt'), (55.172, 'blackassign0040_clean.txt'), (40.625, 'blackassign0041_clean.txt'), (45.373, 'blackassign0042_clean.txt'), (44.655, 'blackassign0043_clean.txt'), (54.545, 'blackassign0044_clean.txt'), (45.543, 'blackassign0045_clean.txt'), (53.061, 'blackassign0046_clean.txt'), (50.986, 'blackassign0047_clean.txt'), (47.26, 'blackassign0048_clean.txt'), (42.576, 'blackassign0050_clean.txt'), (43.26, 'blackassign0051_clean.txt'), (44.331, 'blackassign0052_clean.txt'), (49.688, 'blackassign0053_clean.txt'), (39.487, 'blackassign0054_clean.txt'), (43.144, 'blackassign0055_clean.txt'), (53.674, 'blackassign0056_clean.txt'), (53.289, 'blackassign0057_clean.txt'), (52.326, 'blackassign0058_clean.txt'), (35.143, 'blackassign0059_clean.txt'), (43.038, 'blackassign0060_clean.txt'), (54.908, 'blackassign0061_clean.txt'), (53.527, 'blackassign0062_clean.txt'), (40.308, 'blackassign0063_clean.txt'), (49.084, 'blackassign0064_clean.txt'), (40.612, 'blackassign0065_clean.txt'), (38.967, 'blackassign0066_clean.txt'), (45.768, 'blackassign0067_clean.txt'), (46.705, 'blackassign0068_clean.txt'), (54.237, 'blackassign0069_clean.txt'), (51.147, 'blackassign0070_clean.txt'), (40.887, 'blackassign0071_clean.txt'), (30.943, 'blackassign0072_clean.txt'), (34.609, 'blackassign0073_clean.txt'), (45.601, 'blackassign0074_clean.txt'), (41.801, 'blackassign0075_clean.txt'), (39.198, 'blackassign0076_clean.txt'), (41.504, 'blackassign0077_clean.txt'), (42.367, 'blackassign0078_clean.txt'), (44.057, 'blackassign0079_clean.txt'), (30.025, 'blackassign0080_clean.txt'), (42.617, 'blackassign0081_clean.txt'), (48.997, 'blackassign0082_clean.txt'), (43.662, 'blackassign0083_clean.txt'), (43.875, 'blackassign0084_clean.txt'), (43.192, 'blackassign0085_clean.txt'), (49.938, 'blackassign0086_clean.txt'), (39.834, 'blackassign0087_clean.txt'), (42.616, 'blackassign0088_clean.txt'), (41.834, 'blackassign0089_clean.txt'), (43.444, 'blackassign0090_clean.txt'), (46.436, 'blackassign0091_clean.txt'), (41.463, 'blackassign0092_clean.txt'), (46.154, 'blackassign0093_clean.txt'), (37.333, 'blackassign0094_clean.txt'), (36.111, 'blackassign0095_clean.txt'), (41.166, 'blackassign0096_clean.txt'), (38.795, 'blackassign0097_clean.txt'), (49.302, 'blackassign0098_clean.txt'), (40.074, 'blackassign0099_clean.txt'), (41.255, 'blackassign0100_clean.txt')]\n"
     ]
    }
   ],
   "source": [
    "# calling the function\n",
    "comp_word_percent=[Complex_percent(comp_words, words, file) for comp_words, words, file in zip(complex_num, num_words, clean_files)]\n",
    "print(comp_word_percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e090e1",
   "metadata": {},
   "source": [
    "#### Fog Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7d4371f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function defining fog index\n",
    "def fog_index(avg_sen_len, comp_w_per, file):\n",
    "    fog_index=round(0.4 * (avg_sen_len + comp_w_per),3)\n",
    "    return fog_index, file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "22aee4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(19.055, 'blackassign0001_clean.txt'), (23.634, 'blackassign0002_clean.txt'), (26.864, 'blackassign0003_clean.txt'), (29.473, 'blackassign0004_clean.txt'), (23.702, 'blackassign0005_clean.txt'), (27.541, 'blackassign0006_clean.txt'), (24.071, 'blackassign0007_clean.txt'), (27.088, 'blackassign0008_clean.txt'), (28.472, 'blackassign0009_clean.txt'), (26.546, 'blackassign0010_clean.txt'), (29.521, 'blackassign0011_clean.txt'), (29.963, 'blackassign0012_clean.txt'), (29.967, 'blackassign0013_clean.txt'), (21.864, 'blackassign0014_clean.txt'), (25.943, 'blackassign0015_clean.txt'), (25.943, 'blackassign0016_clean.txt'), (24.501, 'blackassign0017_clean.txt'), (27.88, 'blackassign0018_clean.txt'), (25.358, 'blackassign0019_clean.txt'), (20.339, 'blackassign0020_clean.txt'), (26.688, 'blackassign0021_clean.txt'), (21.243, 'blackassign0022_clean.txt'), (22.179, 'blackassign0023_clean.txt'), (28.131, 'blackassign0024_clean.txt'), (23.722, 'blackassign0025_clean.txt'), (22.038, 'blackassign0026_clean.txt'), (19.221, 'blackassign0027_clean.txt'), (26.038, 'blackassign0028_clean.txt'), (28.514, 'blackassign0029_clean.txt'), (19.245, 'blackassign0030_clean.txt'), (25.29, 'blackassign0031_clean.txt'), (22.057, 'blackassign0032_clean.txt'), (23.047, 'blackassign0033_clean.txt'), (22.69, 'blackassign0034_clean.txt'), (17.238, 'blackassign0035_clean.txt'), (21.732, 'blackassign0037_clean.txt'), (23.575, 'blackassign0038_clean.txt'), (20.938, 'blackassign0039_clean.txt'), (27.869, 'blackassign0040_clean.txt'), (20.237, 'blackassign0041_clean.txt'), (26.032, 'blackassign0042_clean.txt'), (23.774, 'blackassign0043_clean.txt'), (27.771, 'blackassign0044_clean.txt'), (54.117, 'blackassign0045_clean.txt'), (26.737, 'blackassign0046_clean.txt'), (27.868, 'blackassign0047_clean.txt'), (48.104, 'blackassign0048_clean.txt'), (23.316, 'blackassign0050_clean.txt'), (22.621, 'blackassign0051_clean.txt'), (21.342, 'blackassign0052_clean.txt'), (24.616, 'blackassign0053_clean.txt'), (21.795, 'blackassign0054_clean.txt'), (25.673, 'blackassign0055_clean.txt'), (31.903, 'blackassign0056_clean.txt'), (23.959, 'blackassign0057_clean.txt'), (25.23, 'blackassign0058_clean.txt'), (16.603, 'blackassign0059_clean.txt'), (22.482, 'blackassign0060_clean.txt'), (28.641, 'blackassign0061_clean.txt'), (26.231, 'blackassign0062_clean.txt'), (20.663, 'blackassign0063_clean.txt'), (26.186, 'blackassign0064_clean.txt'), (23.092, 'blackassign0065_clean.txt'), (19.238, 'blackassign0066_clean.txt'), (20.67, 'blackassign0067_clean.txt'), (22.011, 'blackassign0068_clean.txt'), (26.752, 'blackassign0069_clean.txt'), (27.688, 'blackassign0070_clean.txt'), (19.155, 'blackassign0071_clean.txt'), (15.63, 'blackassign0072_clean.txt'), (21.263, 'blackassign0073_clean.txt'), (24.584, 'blackassign0074_clean.txt'), (19.56, 'blackassign0075_clean.txt'), (22.685, 'blackassign0076_clean.txt'), (23.44, 'blackassign0077_clean.txt'), (25.275, 'blackassign0078_clean.txt'), (24.353, 'blackassign0079_clean.txt'), (16.116, 'blackassign0080_clean.txt'), (22.109, 'blackassign0081_clean.txt'), (26.39, 'blackassign0082_clean.txt'), (24.565, 'blackassign0083_clean.txt'), (22.539, 'blackassign0084_clean.txt'), (19.879, 'blackassign0085_clean.txt'), (24.878, 'blackassign0086_clean.txt'), (21.442, 'blackassign0087_clean.txt'), (26.902, 'blackassign0088_clean.txt'), (22.152, 'blackassign0089_clean.txt'), (20.522, 'blackassign0090_clean.txt'), (24.961, 'blackassign0091_clean.txt'), (26.304, 'blackassign0092_clean.txt'), (24.702, 'blackassign0093_clean.txt'), (19.498, 'blackassign0094_clean.txt'), (21.644, 'blackassign0095_clean.txt'), (21.457, 'blackassign0096_clean.txt'), (21.666, 'blackassign0097_clean.txt'), (29.276, 'blackassign0098_clean.txt'), (20.975, 'blackassign0099_clean.txt'), (28.191, 'blackassign0100_clean.txt')]\n"
     ]
    }
   ],
   "source": [
    "sent_len=[average_length for average_length,_ in average_len]\n",
    "complex_per=[percent for percent,_ in comp_word_percent]\n",
    "\n",
    "# calling the function\n",
    "f_index=[fog_index(avg_sen_len, comp_w_per, file) for avg_sen_len, comp_w_per, file in zip(sent_len, complex_per, clean_files) ]\n",
    "print(f_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e41d89",
   "metadata": {},
   "source": [
    "#### Average Word Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b00cf3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function counting the average word length\n",
    "def Avg_word_len(num_char, word_count, file):\n",
    "    Avg_word_lens=[]\n",
    "    for num_char, word_count, file in zip(numchar, wordcount, clean_files ):       \n",
    "        Avg_word_len=num_char/word_count\n",
    "        Avg_word_lens.append((Avg_word_len, file))\n",
    "    return Avg_word_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dfa7ab61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpacking the values, creating lists and calling the function\n",
    "wordcount=[count_of_words for _,_,count_of_words in cleaned_files]\n",
    "numchar= [total_characters for total_characters,_ in output]\n",
    "Avg_word=Avg_word_len(numchar, wordcount, clean_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c993421e",
   "metadata": {},
   "source": [
    "#### Personal Pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f22544ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function to search personal pronouns in the text files using regex\n",
    "def personal_pronouns(file_loc):\n",
    "    file_names= os.listdir(file_loc)\n",
    "    p_noun_count=[]\n",
    "    \n",
    "    for file_name in file_names:\n",
    "        file_name_individual=os.path.join(file_loc, file_name)\n",
    "        if file_name.endswith('.txt'):\n",
    "            p_noun=0\n",
    "            with open(file_name_individual,'r',encoding='utf-8') as text:\n",
    "                file=text.read()  \n",
    "                clean_data=file.translate(str.maketrans('', '', string.punctuation))\n",
    "                tokenized_words=word_tokenize(clean_data)\n",
    "                \n",
    "                Personal_pronoun = re.compile(r'\\b(I|we|We|my|My|ours|Ours|us)\\b')\n",
    "\n",
    "                for words in tokenized_words:\n",
    "                    match=re.findall(Personal_pronoun, words)\n",
    "                    p_noun+=len(match)\n",
    "            p_noun_count.append((p_noun, file_name))\n",
    "            \n",
    "    return p_noun_count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d536e025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# providing location of the files here separately as it is to be done on file before removing stop words\n",
    "file_loc=r'C:\\Users\\Admin\\BlackcOffer- Test Assignment'\n",
    "pcount=personal_pronouns(file_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4331e3e",
   "metadata": {},
   "source": [
    "#### Output Updation to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "803a6bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file=r'C:\\Users\\Admin\\Desktop\\Test Assignment\\20211030 Test Assignment\\Output Data Structure.xlsx'\n",
    "df=pd.read_excel(r'C:\\Users\\Admin\\Desktop\\Test Assignment\\20211030 Test Assignment\\Output Data Structure.xlsx')\n",
    "\n",
    "for p_noun, file_name in pcount:\n",
    "    index=df[df['URL_ID']==file_name.replace('.txt', '')].index      \n",
    "\n",
    "    if not index.empty:\n",
    "        df.at[index[0], 'PERSONAL PRONOUNS']=p_noun\n",
    "        \n",
    "df.to_excel(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6f36b991",
   "metadata": {},
   "outputs": [],
   "source": [
    "for positive_score, negative_score, final_file in  pos_neg_score:\n",
    "    index=df[df['URL_ID']==final_file.replace('_clean.txt', '')].index      \n",
    "\n",
    "    if not index.empty:\n",
    "        df.at[index[0], 'POSITIVE SCORE']=positive_score\n",
    "        df.at[index[0], 'NEGATIVE SCORE']=negative_score\n",
    "        \n",
    "        \n",
    "df.to_excel(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "da9a8af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pscore, file_name in polarity_score:\n",
    "        index=df[df['URL_ID']==file_name.replace('_clean.txt', '')].index      \n",
    "        \n",
    "        if not index.empty:\n",
    "            df.at[index[0], 'POLARITY SCORE']=pscore\n",
    "        \n",
    "df.to_excel(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ffa02d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub_score, file_name in subjectivity_score:\n",
    "    index=df[df['URL_ID']==file_name.replace('_clean.txt', '')].index \n",
    "\n",
    "    if not index.empty:\n",
    "        df.at[index[0], 'SUBJECTIVITY SCORE']=sub_score\n",
    "\n",
    "        \n",
    "df.to_excel(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ff9fb51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same output as both are same. Hence updating both with same outputs\n",
    "\n",
    "for average_length, file in average_len:\n",
    "    index=df[df['URL_ID']==file.replace('_clean.txt', '')].index \n",
    "\n",
    "    if not index.empty:\n",
    "        df.at[index[0], 'AVG SENTENCE LENGTH']=average_length\n",
    "        df.at[index[0], 'AVG NUMBER OF WORDS PER SENTENCE']=average_length\n",
    "        \n",
    "\n",
    "        \n",
    "df.to_excel(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "94c32fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for complex_words, clean_file in num:\n",
    "    index=df[df['URL_ID']==clean_file.replace('_clean.txt', '')].index \n",
    "\n",
    "    if not index.empty:\n",
    "        df.at[index[0], 'COMPLEX WORD COUNT']=complex_words\n",
    "\n",
    "        \n",
    "df.to_excel(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1f6db302",
   "metadata": {},
   "outputs": [],
   "source": [
    "for comp_word_per, file in comp_word_percent:\n",
    "    index=df[df['URL_ID']==file.replace('_clean.txt', '')].index \n",
    "\n",
    "    if not index.empty:\n",
    "        df.at[index[0], 'PERCENTAGE OF COMPLEX WORDS']=comp_word_per\n",
    "\n",
    "        \n",
    "df.to_excel(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "92016c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fog_index, file in f_index:\n",
    "    index=df[df['URL_ID']==file.replace('_clean.txt', '')].index \n",
    "\n",
    "    if not index.empty:\n",
    "        df.at[index[0], 'FOG INDEX']=fog_index\n",
    "\n",
    "        \n",
    "df.to_excel(output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0969cc33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bfcb6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
